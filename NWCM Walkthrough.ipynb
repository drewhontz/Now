{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now That's What I Call Music Classifier\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "You are about to follow along with my process for creating a Now That's What I Call Music Classifier that classfies today's hits as either 'now-worthy' or not with **84% accuracy** and returns the 20 selections with highest confidence, but before you do so there are some things you should know.\n",
    "\n",
    "1) The best way to actually run this notebook would be to clone this repo, install Jupyter Notebooks http://jupyter.org/install.html, and execute each cell on your own\n",
    "\n",
    "2) If you follow the advice in Step 1, you can ignore our scraping and correction phase as all dataset are included in the repo\n",
    "\n",
    "If you choose not to follow along in your own notebook, it shouldn't be hard to follow along just by reading each cell.\n",
    "\n",
    "Enjoy!\n",
    "\n",
    "-Drew\n",
    "\n",
    "## 1 - Wrangling the NWCM Data\n",
    "\n",
    "The first data engineering task is to wrangle our data:\n",
    "- Find the link to the NWCM discography and scrape the links of all U.S. series NWCM volumes (exclude NWCM Christmas, Party Anthems, and the like)\n",
    "- Visit each link, scrape the tracklist items: artist, title, and album release date. Store as a dictionary, append to a master list\n",
    "- Authenticate with Spotify as we are going to be querying for our audio features\n",
    "- Clean up our title and artist field a bit to make sure our query returns results (cleaning rules defined in the util python file)\n",
    "- Query for our audio features, popularity, and general naming info from Spotify, add this to the song dictionary item in the data list.\n",
    "- When complete, write this out to a csv\n",
    "\n",
    "- Here things get a little more interesting. During the scraping process if a Spotify query did not return a result, the title and artist were written out to a file named 'corrections.csv'. I can manually search for the track in the Spotify application and add the id. When I run 'make_corrections' the function will read our corrections file and fill in any missing Spotify data to our file. \n",
    "\n",
    "**IMPROVEMENT**: This function was only really meant to be run once so any empty Spotify queries are appended to the corrections file regardless if they already appear in the file or not. Ideally, we don't want to write a line if it's already in the file. Perhaps going forward it would make sense to maintain a database\n",
    "\n",
    "- Our NWCM music data is shaped and cleaned, the next step is to wrangle the billboard data. Billboard data dating further back than our Now albums is available, so I need some sense of how much data to scrape? I decided that adding timedelta, the time lapse between now album release and song release, would be a good idea to figure out: when the best time is to make predictions and how far back before our first now album we have to scrape.\n",
    "    - To accomplish this  I had to clean up our dates and convert them to datetimes. I then created the timedelta by casting the days attribute of the difference of these two datetimes.\n",
    "    - There are many values < 0 due to these popular songs appearing later on 'Greatest Hits' or album rereleases (think Single Edit, or Deluxe Version) so we will just ignore them at this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# importing the custom modules from my util folder\n",
    "from utils.utils import write_list_of_dictionaries_to_file, read_csv_to_list_of_dictionaries,\\\n",
    "    make_corrections, set_timedelta, filter_dataframe_by_year, make_predictions\n",
    "    \n",
    "from utils.wiki_scrape import get_links_from_page, scrape_tracklisting_from_album_link,\\\n",
    "    clean_duration, clean_artist, clean_title\n",
    "    \n",
    "from utils.spotify_utils import create_auth, set_spotify_data, set_spotify_id, set_artist_data,\\\n",
    "    set_audio_features, set_album_data, set_track_data\n",
    "    \n",
    "from utils.billboard_utils import scrape_billboard\n",
    "\n",
    "# importing sklearn modules for use in our classifier section\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell takes ~18 minute to run. I would suggest you download it's resulting file if you are following along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't find: All of Me\n",
      "Can't find: F**kin' Perfect\n",
      "Can't find: Let U Go\n",
      "Can't find: Fresh AZIMIZ\n",
      "Can't find: Baby It's You\n",
      "Can't find: '03 Bonnie & Clyde\n",
      "Can't find: Fast Forward\n",
      "Can't find: Obsession\n",
      "Can't find: Get the Party Started/Sweet Dreams\n",
      "Can't find: Shut the Front Door\n",
      "Can't find: Sugarhigh\n",
      "Can't find: This Summer's Gonna Hurt Like a Mother...\n",
      "Can't find: Be in Love Tonight\n",
      "Can't find: Everybody's Free\n",
      "Can't find: I Care 4 U\n",
      "Can't find: Don't Trust Me\n",
      "Can't find: This or That\n",
      "Can't find: I Believe\n",
      "Can't find: Poisoned with Love\n",
      "Can't find: Thinking for a While\n",
      "Can't find: AM to PM\n",
      "Can't find: Me, Myself & I\n",
      "Can't find: Get Yourself Back Home\n",
      "Can't find: Independent Women Part I\n",
      "Wall time: 18min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# get list of album links\n",
    "url = \"https://en.wikipedia.org/wiki/Now_That%27s_What_I_Call_Music!_discography\"\n",
    "pattern = r\"https://en\\.wikipedia\\.org/wiki/Now_That%27s_What_I_Call_Music!_\\d+_\\(U\\.S\\._series\\)\"\n",
    "url_prepend = \"https://en.wikipedia.org\"\n",
    "\n",
    "album_links = get_links_from_page(url, pattern, url_prepend)\n",
    "\n",
    "# appending the original album link since it doesn't fit our regex pattern\n",
    "album_links.append(\"https://en.wikipedia.org/wiki/Now_That%27s_What_I_Call_Music!_(original_U.S._album)\")\n",
    "\n",
    "# scrape each tracklisting, extending the data list each album\n",
    "now = []\n",
    "for album in album_links:\n",
    "    now.extend(scrape_tracklisting_from_album_link(album))\n",
    "\n",
    "# Authenicating with Spotify\n",
    "sp = create_auth()\n",
    "\n",
    "# clean fields to improve spotify query accuracy\n",
    "for song in now:\n",
    "    song['duration'] = clean_duration(song['duration'])\n",
    "    song['title'] = clean_title(song['title'])\n",
    "    song['artist'] = clean_artist(song['artist'])\n",
    "    # Add clean date function here to set date format to datetime\n",
    "    set_spotify_data(song, sp)\n",
    "\n",
    "# write to csv\n",
    "write_list_of_dictionaries_to_file(now, \"NWCM_Spotify_Features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated All of Me by John Legend\n"
     ]
    }
   ],
   "source": [
    "# read and update our file with any corrections listed in the corrections file\n",
    "make_corrections(\"NWCM_Spotify_Features.csv\", \"corrections.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - How much training data do we need?\n",
    "\n",
    "I can scrape back as far as I'd like to but songs on the billboard charts in say, 1941, are unlikely to be useful in classifying today's hits from 'now-worthy' hits. I decided it would be a good question to answer, what is the lapse in time between a song's release and it's appearance on a Now album. This should provide me with not only how far much data I should collect for training, but how far I need to go back when making prediction as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.428571428571427"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update our data from the manually updated file\n",
    "now = read_csv_to_list_of_dictionaries(\"NWCM_Spotify_Features.csv\")\n",
    "\n",
    "# set timedelta column\n",
    "for song in now:\n",
    "    set_timedelta(song)\n",
    "    \n",
    "# read into a dataframe, drop na values in timedelta, locate only valid times (i.e. > 0 days difference)\n",
    "# find the median number of days and return this as a week \n",
    "nowdf = pd.DataFrame(now)\n",
    "nowdf.timedelta\\\n",
    "    .dropna(how='any', inplace=False)\\\n",
    "    .loc[nowdf.timedelta >= 0]\\\n",
    "    .median()/7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like 31 weeks before our first now album release is a good cut off for our training data collection.\n",
    "**Question**: I am assuming with the greater access to music today (in relation to 1998) that songs are not deemed 'popular' as long as they used to be. A good way to examine this question would be to look at how this delta has changed over the course of our Now history.\n",
    "\n",
    "# 3 - Scraping our Billboard data\n",
    "\n",
    "Rather than scraping all the way back 31 weeks before the first album as I did during the first pass of this project, I will let you in on a little inside knowledge I gained during the feature selection phase. \n",
    "\n",
    "Popularity is key. Popularity also degrades as total daily play count on Spotify declines. \n",
    "\n",
    "This is a huge limitation on the amount of data we have collected, as songs on now albums have much lower popularity scores than they would have had when they were being selected to appear on that season's release. In essence, unless a song from now 30 is still extremely popular today, we will be training our model on popularity features **today** when we really need popularity scores from when the NWCM album was released. For this reason, we will only be using data from 2016 onward.\n",
    "\n",
    "This problem could be remedied if we had a figure such as 'peek popularity' or a series of historical popularity scores for each track. While this in currently unavailable, if I want to improve the model going forward, this is a feature I could track, it wouldn't make our model any stronger today but in a few years time perhaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell takes ~16 minutes to run. I would suggest you download it's resulting file if you are following along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data from: 2017-02-18\n",
      "Getting data from: 2017-02-11\n",
      "Getting data from: 2017-02-04\n",
      "Getting data from: 2017-01-28\n",
      "Getting data from: 2017-01-21\n",
      "Getting data from: 2017-01-14\n",
      "Getting data from: 2017-01-07\n",
      "Getting data from: 2016-12-31\n",
      "Getting data from: 2016-12-24\n",
      "Getting data from: 2016-12-17\n",
      "Getting data from: 2016-12-10\n",
      "Getting data from: 2016-12-03\n",
      "Getting data from: 2016-11-26\n",
      "Getting data from: 2016-11-19\n",
      "Getting data from: 2016-11-12\n",
      "Getting data from: 2016-11-05\n",
      "Getting data from: 2016-10-29\n",
      "Getting data from: 2016-10-22\n",
      "Getting data from: 2016-10-15\n",
      "Getting data from: 2016-10-08\n",
      "Getting data from: 2016-10-01\n",
      "Getting data from: 2016-09-24\n",
      "Getting data from: 2016-09-17\n",
      "Getting data from: 2016-09-10\n",
      "Getting data from: 2016-09-03\n",
      "Getting data from: 2016-08-27\n",
      "Getting data from: 2016-08-20\n",
      "Getting data from: 2016-08-13\n",
      "Getting data from: 2016-08-06\n",
      "Getting data from: 2016-07-30\n",
      "Getting data from: 2016-07-23\n",
      "Getting data from: 2016-07-16\n",
      "Getting data from: 2016-07-09\n",
      "Getting data from: 2016-07-02\n",
      "Getting data from: 2016-06-25\n",
      "Getting data from: 2016-06-18\n",
      "Getting data from: 2016-06-11\n",
      "Getting data from: 2016-06-04\n",
      "Getting data from: 2016-05-28\n",
      "Getting data from: 2016-05-21\n",
      "Getting data from: 2016-05-14\n",
      "Getting data from: 2016-05-07\n",
      "Getting data from: 2016-04-30\n",
      "Getting data from: 2016-04-23\n",
      "Getting data from: 2016-04-16\n",
      "Getting data from: 2016-04-09\n",
      "Getting data from: 2016-04-02\n",
      "Getting data from: 2016-03-26\n",
      "Getting data from: 2016-03-19\n",
      "Getting data from: 2016-03-12\n",
      "Getting data from: 2016-03-05\n",
      "Getting data from: 2016-02-27\n",
      "Getting data from: 2016-02-20\n",
      "Getting data from: 2016-02-13\n",
      "Getting data from: 2016-02-06\n",
      "Getting data from: 2016-01-30\n",
      "Getting data from: 2016-01-23\n",
      "Getting data from: 2016-01-16\n",
      "Getting data from: 2016-01-09\n",
      "Getting data from: 2016-01-02\n",
      "Getting data from: 2015-12-26\n",
      "Getting data from: 2015-12-19\n",
      "Getting data from: 2015-12-12\n",
      "Getting data from: 2015-12-05\n",
      "Getting data from: 2015-11-28\n",
      "Getting data from: 2015-11-21\n",
      "Getting data from: 2015-11-14\n",
      "Getting data from: 2015-11-07\n",
      "Getting data from: 2015-10-31\n",
      "Getting data from: 2015-10-24\n",
      "Getting data from: 2015-10-17\n",
      "Getting data from: 2015-10-10\n",
      "Getting data from: 2015-10-03\n",
      "Getting data from: 2015-09-26\n",
      "Getting data from: 2015-09-19\n",
      "Getting data from: 2015-09-12\n",
      "Getting data from: 2015-09-05\n",
      "Getting data from: 2015-08-29\n",
      "Getting data from: 2015-08-22\n",
      "Getting data from: 2015-08-15\n",
      "Getting data from: 2015-08-08\n",
      "Getting data from: 2015-08-01\n",
      "Getting data from: 2015-07-25\n",
      "Getting data from: 2015-07-18\n",
      "Getting data from: 2015-07-11\n",
      "Getting data from: 2015-07-04\n",
      "Getting data from: 2015-06-27\n",
      "Getting data from: 2015-06-20\n",
      "Getting data from: 2015-06-13\n",
      "Getting data from: 2015-06-06\n",
      "Getting data from: 2015-05-30\n",
      "Getting data from: 2015-05-23\n",
      "Getting data from: 2015-05-16\n",
      "Getting data from: 2015-05-09\n",
      "Getting data from: 2015-05-02\n",
      "Getting data from: 2015-04-25\n",
      "Getting data from: 2015-04-18\n",
      "Getting data from: 2015-04-11\n",
      "Getting data from: 2015-04-04\n",
      "Getting data from: 2015-03-28\n",
      "Getting data from: 2015-03-21\n",
      "Getting data from: 2015-03-14\n",
      "Getting data from: 2015-03-07\n",
      "Getting data from: 2015-02-28\n",
      "Wall time: 12min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Let's scrape back two years to start\n",
    "billboard = scrape_billboard(104)\n",
    "\n",
    "# Write to csv\n",
    "write_list_of_dictionaries_to_file(billboard, \"Billboard_Spotify_Features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Forming the Training Data Set\n",
    "\n",
    "To create our training data set we now need to first merge our billboard and now data.\n",
    "\n",
    "The process is as follows:\n",
    "\n",
    "- read files to dataframes, add respective 'now' labels\n",
    "- append the dataframes and set results to training\n",
    "- clean up columns so they can be passed on to our feature investigation phase\n",
    "- We will need to subset this dataset by album release year to accomdate for the popularity decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reading in our csv files\n",
    "now = pd.read_csv(\"NWCM_Spotify_Features.csv\")\n",
    "billboard = pd.read_csv(\"Billboard_Spotify_Features.csv\")\n",
    "\n",
    "# add now labels\n",
    "now['now'] = 1\n",
    "billboard['now'] = 0\n",
    "\n",
    "# append dataframes, project only the columns we will be using in training and drop na\n",
    "training = now.append(billboard)\n",
    "training = training[['track_id', 'acousticness', 'album_popularity', 'artist_popularity',\n",
    " 'album_release_date', 'danceability', 'duration_ms', 'energy',\n",
    " 'instrumentalness', 'key', 'liveness', 'loudness', 'mode', 'now',\n",
    " 'speechiness', 'tempo', 'track_popularity', 'valence']]\n",
    "\n",
    "# an improvement in the future would be to fill na in now and billboard with median values, respectively\n",
    "training.dropna(how='any', inplace=True)\n",
    "\n",
    "# Filter training dataset by album release year\n",
    "training = filter_dataframe_by_year(training, 'album_release_date', 2015)\n",
    "\n",
    "# Dropping duplicate track_id's (i.e removing billboard songs that appear on a now cd)\n",
    "training.sort_values('track_id', inplace=True)\n",
    "training = training.drop(training.loc[training.track_id == training.track_id.shift()].index)\n",
    "\n",
    "# Write to csv\n",
    "training.to_csv(\"TrainingData.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Creating our Classifier\n",
    "\n",
    "In this section of the project we will select our features and form the training data, train a few different classifiers, evaluate them on accuracy, and finally make our predictions for the next now album!\n",
    "\n",
    "## Read Training Data, Select Features, and Create Splits\n",
    "\n",
    "Here we are reading in our training set, selecting our 3 best features, and training 3 different classifiers which we will evaluate on their accuracy.\n",
    "\n",
    "I ended up selecting the KNN classifier as consistently has the highest accuracy.\n",
    "\n",
    "Something to improve in the next iteration would be making use of the pipeline and GridSearchCV modules in sklearn. For this project I wanted to experiment and turn some 'knobs' myself but it would be much wiser to automate this using the tools provided in sklearn going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features in use: ['album_popularity', 'artist_popularity', 'energy', 'loudness', 'track_popularity']\n",
      "Decision Tree\tScore: 0.812865497076\n",
      "SVM\tScore: 0.842105263158\n",
      "KNN\tScore: 0.847953216374\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "data = pd.read_csv(\"TrainingData.csv\")\n",
    "\n",
    "# Filtering our now from our data\n",
    "x = data[[ 'acousticness', 'album_popularity', 'artist_popularity', 'danceability', 'duration_ms',\n",
    " 'energy', 'instrumentalness', 'key', 'liveness', 'loudness', 'mode', 'speechiness', 'tempo',\n",
    " 'track_popularity', 'valence']]\n",
    "y = data['now']\n",
    "\n",
    "# Selecting our features\n",
    "k = 5\n",
    "x_new = SelectKBest(score_func=f_classif, k=k).fit_transform(x,y)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_new, y, train_size = 0.8)\n",
    "\n",
    "feature_names = list(x.columns[list(SelectKBest(score_func=f_classif, k=k).fit(x, y).get_support())])\n",
    "print \"Features in use: \" + str(feature_names)\n",
    "\n",
    "# Creating 3 Classifiers to test\n",
    "classifiers = [\n",
    "    {'name': \"Decision Tree\", 'classifier' : DecisionTreeClassifier(criterion='entropy')},\n",
    "    {'name': 'SVM', 'classifier' : SVC(C=10)},\n",
    "    {'name': 'KNN', 'classifier': KNeighborsClassifier(n_neighbors=11, weights='uniform')}\n",
    "]\n",
    "\n",
    "for clf in classifiers:\n",
    "    clf['classifier'].fit(x_train, y_train)\n",
    "    print \"{}\\tScore: {}\".format(clf['name'], clf['classifier'].score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our model predicts with **~84% accuracy**!\n",
    "\n",
    "Now let's see what songs will appear on the next album!\n",
    "\n",
    "# 6 - Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data from: 2017-02-18\n",
      "Getting data from: 2017-02-11\n",
      "Getting data from: 2017-02-04\n",
      "Getting data from: 2017-01-28\n",
      "Getting data from: 2017-01-21\n",
      "Getting data from: 2017-01-14\n",
      "Getting data from: 2017-01-07\n",
      "Getting data from: 2016-12-31\n",
      "Getting data from: 2016-12-24\n",
      "Getting data from: 2016-12-17\n",
      "Getting data from: 2016-12-10\n",
      "Getting data from: 2016-12-03\n",
      "Getting data from: 2016-11-26\n",
      "Getting data from: 2016-11-19\n",
      "Getting data from: 2016-11-12\n",
      "Getting data from: 2016-11-05\n",
      "Getting data from: 2016-10-29\n",
      "Getting data from: 2016-10-22\n",
      "Getting data from: 2016-10-15\n",
      "Getting data from: 2016-10-08\n",
      "Getting data from: 2016-10-01\n",
      "Getting data from: 2016-09-24\n",
      "Getting data from: 2016-09-17\n",
      "Getting data from: 2016-09-10\n",
      "Getting data from: 2016-09-03\n",
      "Getting data from: 2016-08-27\n",
      "Getting data from: 2016-08-20\n",
      "Getting data from: 2016-08-13\n",
      "Getting data from: 2016-08-06\n",
      "Getting data from: 2016-07-30\n",
      "Getting data from: 2016-07-23\n",
      "\n",
      "Predictions for the Next Now album:\n",
      "\n",
      "Meek Mill Litty (feat. Tory Lanez) 0.727272727273\n",
      "PIKOTARO PPAP (Pen Pineapple Apple Pen) - Long Version 0.727272727273\n",
      "Coldplay Hymn For The Weekend 0.727272727273\n",
      "Sundance Head Darlin' Don't Go 0.636363636364\n",
      "Meghan Trainor Me Too 0.636363636364\n",
      "Flo Rida My House 0.636363636364\n",
      "Sia Cheap Thrills 0.545454545455\n",
      "Marshmello Alone 0.545454545455\n",
      "Lil Uzi Vert You Was Right 0.545454545455\n",
      "Kodak Black No Flockin 0.545454545455\n",
      "Fifth Harmony That's My Girl 0.545454545455\n",
      "Kehlani Gangsta 0.545454545455\n",
      "Fifth Harmony All In My Head (Flex) 0.545454545455\n",
      "Young Thug Wyclef Jean 0.545454545455\n",
      "Disturbed The Sound Of Silence 0.545454545455\n",
      "Wall time: 4min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#  get last 31 weeks of popular music from billboard\n",
    "recent_popular_songs = scrape_billboard(31)\n",
    "\n",
    "#  get audio features, project to our predictive features\n",
    "pop_songs_df = pd.DataFrame(recent_popular_songs)\n",
    "pop_songs_df.dropna(how='any', inplace=True)\n",
    "\n",
    "# make predictions on our current popular songs using KNN classifier\n",
    "album_predictions = make_predictions(pop_songs_df, classifiers[2]['classifier'], feature_names)\n",
    "\n",
    "print \"\\nPredictions for the Next Now album:\\n\"\n",
    "# print our predictions\n",
    "for song in album_predictions:\n",
    "    # print artist, title, and confidence\n",
    "    print song[0], song[1], song[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Improvements in the Future\n",
    "\n",
    "### Current Problems and Limitations\n",
    "\n",
    "- Predictions could theoretically include songs that were released on NWCM albums in the last 9 months\n",
    "- Predictions limited to what has appeared on Billboard Top 100 in 9 months\n",
    "- Predictions to not take into account proximity to next Now release. I.e. If the next Now album is due in a week, we will call this release n, the predictions are more likely to be suitable for the n+1 release. This classifier really answers the question, **'Of the Billboard Top 100 charts for the past 9 months, which 20 songs are most likely to appear on a Now CD in the future?`**\n",
    "- Lack of reliable features. We would really want a track's peak album, artist, and track popularity instead of today's popularity\n",
    "\n",
    "### Data and Features\n",
    "\n",
    "- Timedelta; I computed the difference in time between the NWCM album and original album release but there were several issues with determining the actual release time of the song. Spending time correcting these errors and determing a methodology for finding Billboard song's time delta is a costly effort but it is highest priority in improving this model\n",
    "- Max popularity as a feature. The biggest problem with my classifier is lack of quality features. I went from experimenting with ~15 features to just 3; and those 3 are all popularity based. Spotify popularity changes over time, making it an unreliable feature for older tracks and thus limiting the amount of our training data we could have used. If I were to host this on a server and preserve the popularity for songs at the time they appear on a Now album, I could create a much stronger classifier going forward (either that or Spotify could release a historical popularity endpoint in their API).\n",
    "- Experimentation with other features: Some ideas I have had for other features to wrangle during the course of this project were:\n",
    "    - Does this song appear on a movie soundtrack?\n",
    "    - Number of albums this song appears on in Spotify\n",
    "    - Count of 'alternate' versions on Spotify (i.e. what is the count of 'remix' or 'radio edit' type versions of this song on spotify\n",
    "- **Imputation**: Taylor Swift songs account for 1.3% of our Now data and every data droplet is important. I mistakenly dropped songs from both our Billboard and NWCM scrapes that did not feature a spotify id. I would have rather filled these values with median values.\n",
    "\n",
    "### Algorithms\n",
    "\n",
    "- I tried out a few popular machine learning algorithms here and chose that which had the best performance. I did not however pursue and ensemble methods (combining multiple algorithms to create one, stronger model)\n",
    "- Collaborative filtering, I know a lot of recommendation systems use a collaborative filtering model, perhaps another day I will revisit this project and try to fit it to this type of algorithm\n",
    "- Utilizing sklearn module GridSearchCV for best tuning parameters (I ran experiments myself but I would like to make my experiments more reproducible and automate the process next go-around)\n",
    "\n",
    "### Organization\n",
    "\n",
    "- I typically scrape data through a series of 'run-once' type scripts but this is not a good habit if you want to have a clean notebook; further improving scraping functions so they are 'run-once' would be nice.\n",
    "- Using a data store other than multiple csv files could help\n",
    "\n",
    "### Presentation\n",
    "\n",
    "- While the purpose of this project was to have fun, experiment with machine learning, and learn something new, I feel this project could really benefit from a simple front end where users can see the prediction process unfold in front of them.\n",
    "- Hosting this on a server could also allow better data storage strategies (i.e. having one master table that we read from and update with popularity and new Billboard songs each week so we don't have to scrape the most recent data for a prediction)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:DAND]",
   "language": "python",
   "name": "conda-env-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
